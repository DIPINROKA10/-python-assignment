{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPINROKA10/-python-assignment/blob/main/day2_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH25aXASfNZd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWN27pNafNsZ"
      },
      "source": [
        "##Day 2 Hands-On Lab:\n",
        "Mastering Prompt Engineering: This notebook covers the practical application of advanced Prompt Engineering frameworks like R.O.L.E.S. and Chain of Thought (CoT).\n",
        "\n",
        "We will compare the results of poorly-structured prompts against engineered prompts using LangChain and two powerful models: OpenAI (GPT-4o) and Gemini model\n",
        "\n",
        "Setup & Environment Configuration: We need to install the necessary libraries and set up our API keys.\n",
        "\n",
        "We recommend using Colab Secrets for secure storage of your OPENAI_API_KEY and GEMINI_API_KEY.#\n",
        "\n",
        "Install LangChain and necessary dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai==0.8.2\n"
      ],
      "metadata": {
        "id": "Fej1feB5n0kn",
        "outputId": "8956c228-0404-41aa-ef12-c49c1b2598e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-generativeai==0.8.2\n",
            "  Using cached google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting google-ai-generativelanguage==0.6.10 (from google-generativeai==0.8.2)\n",
            "  Downloading google_ai_generativelanguage-0.6.10-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.2) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.2) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.2) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.2) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.2) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.2) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai==0.8.2) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai==0.8.2) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai==0.8.2) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.2) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.2) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.2) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.2) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.2) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.2) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai==0.8.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai==0.8.2) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai==0.8.2) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai==0.8.2) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai==0.8.2) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai==0.8.2) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai==0.8.2) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai==0.8.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai==0.8.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai==0.8.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai==0.8.2) (2025.11.12)\n",
            "Downloading google_generativeai-0.8.2-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.4/153.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.6.10-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-ai-generativelanguage, google-generativeai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.8.5\n",
            "    Uninstalling google-generativeai-0.8.5:\n",
            "      Successfully uninstalled google-generativeai-0.8.5\n",
            "Successfully installed google-ai-generativelanguage-0.6.10 google-generativeai-0.8.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "7750628ed3ce42b38bf4d3c95841dfac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcOJ1kDcfWqG",
        "outputId": "bdc23318-1f6c-4d44-8fc4-496a6753210c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.2.5\n",
            "  Downloading langchain-0.2.5-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting langchain-core==0.2.33\n",
            "  Downloading langchain_core-0.2.33-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-google-genai==2.0.2\n",
            "  Downloading langchain_google_genai-2.0.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting google-generativeai==0.8.2\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (3.13.2)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.5)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.5)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from langchain==0.2.5)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.5) (2.32.4)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.2.5)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.2.33) (1.33)\n",
            "Collecting packaging<25,>=23.2 (from langchain-core==0.2.33)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.2.33) (4.15.0)\n",
            "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install langchain-core==0.2.33, langchain-google-genai==2.0.2 and langchain==0.2.5 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested langchain-core==0.2.33\n",
            "    langchain 0.2.5 depends on langchain-core<0.3.0 and >=0.2.7\n",
            "    langchain-google-genai 2.0.2 depends on langchain-core<0.4 and >=0.3.13\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.2.5 langchain-core==0.2.33 langchain-google-genai==2.0.2 google-generativeai==0.8.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ufdvkWEfraa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "c5cc40f5-ab53-49cc-dd6e-b59774a1e4d0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_google_genai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1450369059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_google_genai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatGoogleGenerativeAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStrOutputParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_google_genai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from google.colab import userdata\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tdwZq43ftEe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- API Key Setup ---\n",
        "# Load environment variables (assumes using Colab Secrets or a .env file)\n",
        "# If using Colab Secrets, click the key icon on the left panel.\n",
        "# Variables must be named OPENAI_API_KEY and GEMINI_API_KEY.\n",
        "\n",
        "# Get API keys from environment variables\n",
        "\n",
        "gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "if not gemini_api_key:\n",
        "    print(\"Warning: GEMINI_API_KEY not found. Gemini model will not run.\")\n",
        "\n",
        "# openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "# if not openai_api_key:\n",
        "#     print(\"Warning: OPENAI_API_KEY not found. OpenAI model will not run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "HSLM9G7xf6iK",
        "outputId": "8e034e82-3a67-4392-90fa-e5263affa7d2"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'langchain' has no attribute 'verbose'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3116793850.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 2. Google Gemini Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Using gemini-2.5-flash for a highly performant and cost-effective alternative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m gemini_model = ChatGoogleGenerativeAI(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini-2.0-flash-lite-001\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgoogle_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgemini_api_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419  # Intentional blank docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/base.py\u001b[0m in \u001b[0;36m_get_verbosity\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/globals.py\u001b[0m in \u001b[0;36mget_verbose\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# deprecation warnings directing them to use `set_verbose()` when they\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# import `langchain.verbose`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mold_verbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mold_verbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'langchain' has no attribute 'verbose'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- Model Initialization ---\n",
        "# 1. OpenAI Model\n",
        "# Using a powerful model like gpt-4o for complex tasks\n",
        "# openai_model = ChatOpenAI(\n",
        "#     model=\"gpt-4o-mini\",\n",
        "#     api_key=openai_api_key,\n",
        "#     temperature=0.3\n",
        "# )\n",
        "\n",
        "# 2. Google Gemini Model\n",
        "# Using gemini-2.5-flash for a highly performant and cost-effective alternative\n",
        "gemini_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-lite-001\",\n",
        "    google_api_key=gemini_api_key,\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "print(\"Setup complete. Models initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc8EQJNNgUWu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8jwpp48gVCy"
      },
      "source": [
        "\n",
        "\n",
        "2. Advanced Reasoning: Chain of Thought (CoT) The CoT technique involves adding the phrase \"Let's think step by step\" to the prompt. This forces the LLM to structure its reasoning before providing a final answer, dramatically improving accuracy in logic and mathematical problems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llGJaPrEgjhe",
        "outputId": "7a00c3b6-f3a0-45b5-d198-c5660c41a96a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- GPT Standard Response ---\n",
            "If all but 9 goats run away, that means 9 goats are still with the farmer. Therefore, the number of goats left is 9.\n",
            "\n",
            "--- GPT-4o CoT Response (Correct) ---\n",
            "Let's break down the problem step by step:\n",
            "\n",
            "1. The farmer starts with 17 goats.\n",
            "2. The phrase \"All but 9 run away\" means that all the goats except for 9 have run away.\n",
            "3. Therefore, if 9 goats did not run away, that means those 9 goats are still with the farmer.\n",
            "\n",
            "So, the number of goats left with the farmer is **9**.\n"
          ]
        }
      ],
      "source": [
        "user_query = \"\"\"\n",
        "A farmer has 17 goats. All but 9 run away. How many goats are left?\n",
        "\"\"\"\n",
        "\n",
        "# --- Standard Prompt  ---\n",
        "standard_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"{query}\")\n",
        "])\n",
        "\n",
        "chain_standard = standard_prompt | gemini_model | StrOutputParser()\n",
        "print(\"--- GPT Standard Response ---\")\n",
        "print(chain_standard.invoke({\"query\": user_query}))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIeh4zIhCk2T"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Chain of Thought (CoT) Prompt (Reasoning) ---\n",
        "cot_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"Let's think step by step to find the correct answer. {query}\")\n",
        "])\n",
        "chain_cot = cot_prompt | gemini_model | StrOutputParser()\n",
        "print(\"\\n--- GPT-4o CoT Response (Correct) ---\")\n",
        "print(chain_cot.invoke({\"query\": user_query}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrsxs1x8glan"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvXa3hZDg4sf"
      },
      "source": [
        "**3. The R.O.L.E.S. Prompt Engineering Framework : **\n",
        "\n",
        "R.O.L.E.S. ensures every critical aspect of the desired output is explicitly defined, reducing ambiguity and improving consistency.\n",
        "\n",
        "Role: The identity the LLM adopts (e.g., expert, beginner).Sets expertise,tone, and knowledge boundaries.\n",
        "\n",
        "Objective: The core task to accomplish (e.g., summarize, critique, generate).Defines the purpose of the output.\n",
        "\n",
        "Limitations: Constraints on length, format, or content (e.g., max 50 words, exclude jargon).Ensures practical, usable results.\n",
        "\n",
        "Examples: Few-shot learning examples (Input $\\to$ Output pairs).Guides model on specific required style or format.\n",
        "\n",
        "Style: Tone, language complexity, and output format (e.g., professional, JSON, Markdown).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jPNtISJj-Cd"
      },
      "source": [
        "## Example 1 : Write a blog post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rybGcxVFjzLG",
        "outputId": "c7678f8d-29da-4292-d311-9209151be0bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---  PROMPT (GPT-4o) ---\n",
            "# The Rise of Artificial Intelligence: Transforming Our World\n",
            "\n",
            "In recent years, artificial intelligence (AI) has transitioned from a niche area of research to a transformative force that is reshaping industries, enhancing everyday life, and challenging our understanding of what machines can achieve. As we stand on the brink of an AI-driven future, it’s essential to explore the implications, benefits, and challenges that come with this revolutionary technology.\n",
            "\n",
            "## What is Artificial Intelligence?\n",
            "\n",
            "At its core, artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. This encompasses a wide range of technologies, including machine learning, natural language processing, computer vision, and robotics. AI systems can analyze vast amounts of data, recognize patterns, make predictions, and even engage in conversations, all of which contribute to their growing capabilities.\n",
            "\n",
            "## The Impact of AI on Industries\n",
            "\n",
            "### 1. Healthcare\n",
            "\n",
            "AI is making significant strides in healthcare, from diagnostics to treatment recommendations. Machine learning algorithms can analyze medical images with remarkable accuracy, assisting radiologists in detecting conditions like cancer at earlier stages. AI-driven predictive analytics can help healthcare providers anticipate patient needs, optimize treatment plans, and improve patient outcomes.\n",
            "\n",
            "### 2. Finance\n",
            "\n",
            "In the financial sector, AI is revolutionizing risk assessment, fraud detection, and customer service. Algorithms can analyze transaction patterns in real-time to flag suspicious activities, while robo-advisors provide personalized investment advice based on individual risk profiles. This not only enhances security but also democratizes access to financial services.\n",
            "\n",
            "### 3. Transportation\n",
            "\n",
            "The advent of autonomous vehicles is one of the most talked-about applications of AI. Companies like Tesla, Waymo, and others are developing self-driving cars that promise to reduce accidents, ease traffic congestion, and transform urban mobility. AI is also optimizing logistics and supply chain management, ensuring that goods are delivered efficiently and cost-effectively.\n",
            "\n",
            "### 4. Education\n",
            "\n",
            "AI is personalizing education by providing tailored learning experiences for students. Intelligent tutoring systems can adapt to individual learning styles and paces, offering customized resources and feedback. This technology has the potential to bridge educational gaps and make quality learning accessible to all.\n",
            "\n",
            "## The Benefits of AI\n",
            "\n",
            "The benefits of AI are manifold. By automating routine tasks, AI frees up human workers to focus on more complex and creative endeavors. This can lead to increased productivity and innovation across various sectors. Additionally, AI can enhance decision-making by providing data-driven insights, allowing businesses to respond more effectively to market changes.\n",
            "\n",
            "Moreover, AI has the potential to address some of the world’s most pressing challenges, such as climate change, food security, and healthcare disparities. For instance, AI algorithms can optimize energy consumption in smart grids, predict crop yields, and streamline resource allocation in disaster response scenarios.\n",
            "\n",
            "## The Challenges Ahead\n",
            "\n",
            "Despite its promise, the rise of AI is not without challenges. Ethical concerns surrounding data privacy, algorithmic bias, and job displacement are at the forefront of discussions about AI’s future. As machines become more autonomous, questions about accountability and transparency arise. Who is responsible when an AI system makes a mistake? How do we ensure that AI systems are fair and unbiased?\n",
            "\n",
            "Furthermore, the rapid pace of AI development poses regulatory challenges. Policymakers must strike a balance between fostering innovation and protecting citizens from potential harms. This requires collaboration between technologists, ethicists, and lawmakers to create frameworks that ensure responsible AI deployment.\n",
            "\n",
            "## The Future of AI\n",
            "\n",
            "As we look to the future, the potential of AI seems limitless. Continued advancements in technology will likely lead to even more sophisticated AI systems capable of tackling complex problems. However, it is crucial that we approach this future with caution and foresight.\n",
            "\n",
            "Investing in education and reskilling programs will be essential to prepare the workforce for an AI-enhanced economy. Additionally, fostering a culture of ethical AI development will help ensure that the benefits of this technology are shared equitably across society.\n",
            "\n",
            "In conclusion, artificial intelligence is not just a technological trend; it is a paradigm shift that has the power to redefine our world. By embracing its potential while addressing its challenges, we can harness AI to create a brighter, more equitable future for all. The journey ahead may be complex, but with thoughtful stewardship, the possibilities are truly exciting.\n"
          ]
        }
      ],
      "source": [
        "def generate_roles_prompt(role, objective, limitations, style, query):\n",
        "    \"\"\"Generates a structured prompt based on the R.O.L.E.S. framework.\"\"\"\n",
        "    prompt_template = f\"\"\"\n",
        "    --- INSTRUCTIONS (R.O.L.E.S. FRAMEWORK) ---\n",
        "    ROLE: {role}\n",
        "    OBJECTIVE: {objective}\n",
        "    LIMITATIONS: {limitations}\n",
        "    STYLE: {style}\n",
        "    --- USER QUERY ---\n",
        "    {query}\n",
        "    \"\"\"\n",
        "    return prompt_template\n",
        "\n",
        "# Define a simple chain template for easy switching between models\n",
        "def create_roles_chain(model):\n",
        "    prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "    return prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "blog_prompt = \"Write a blog post about AI.\"\n",
        "print(\"---  PROMPT (GPT-4o) ---\")\n",
        "print(create_roles_chain(openai_model).invoke({\"input\": blog_prompt}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4aYZG6vkRwO",
        "outputId": "793eb30f-d710-4489-908c-3c5cec398d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\n",
            "## Unlock the Power of Language: Introducing R.O.L.E.S.\n",
            "\n",
            "Hello, fellow marketing mavens and tech enthusiasts! Are you ready to revolutionize how you interact with sophisticated language models? We're on the cusp of a new era, and mastering the art of prompt engineering is your key to unlocking unprecedented creative and strategic potential. Forget generic requests – it's time to craft prompts that truly *perform*.\n",
            "\n",
            "That's where R.O.L.E.S. comes in. This isn't just another framework; it's your secret weapon for eliciting the precise, high-quality outputs you need to dominate the market. R.O.L.E.S. is a structured approach, designed to guide you through the process of crafting prompts that are both clear and effective.\n",
            "\n",
            "Think of it as a meticulously crafted blueprint. Each letter represents a crucial element: **R**ole, **O**bjective, **L**imitation, **E**xample, and **S**tyle. By systematically defining these components, you'll transform vague instructions into laser-focused directives.\n",
            "\n",
            "Imagine: crafting compelling ad copy that resonates with your target audience, generating innovative campaign ideas in seconds, or even automating complex market research tasks. With R.O.L.E.S., these possibilities become reality.\n",
            "\n",
            "Over the coming weeks, we'll delve deeper into each component of the framework. Get ready to learn how to assign roles, define objectives, set boundaries, provide examples, and master the art of stylistic control. Prepare to witness a paradigm shift in your workflow. Let's get started!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT ---\n",
        "roles_blog_prompt = generate_roles_prompt(\n",
        "    role=\"A seasoned Marketing Director and AI expert.\",\n",
        "    objective=\"Write a compelling, 300-word introduction to the R.O.L.E.S. prompt engineering framework.\",\n",
        "    limitations=\"Maximum 300 words. Must not use the word 'chatbot' or 'AI helper'.\",\n",
        "    style=\"Enthusiastic, engaging, and structured as Markdown with a H2 title.\",\n",
        "    query=\"Write the post now.\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": roles_blog_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XrTp6kaj3gE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI1LMOqAk68K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1fKMGPak7ZD"
      },
      "source": [
        "Prompt Set 2: Code Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axaun9LklKTg",
        "outputId": "9e1daa80-f6f8-46b0-86cb-0537bf5c77d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- PROMPT (Gemini 2.5 Flash) ---\n",
            "```python\n",
            "def sum_list(l):\n",
            "  \"\"\"\n",
            "  Calculates the sum of all numbers in a list.\n",
            "\n",
            "  Args:\n",
            "    l: A list of numbers.\n",
            "\n",
            "  Returns:\n",
            "    The sum of the numbers in the list.\n",
            "  \"\"\"\n",
            "  total = 0\n",
            "  for i in l:\n",
            "    total += i\n",
            "  return total\n",
            "```\n",
            "\n",
            "**What was wrong (and why the fix is what it is):**\n",
            "\n",
            "*   **The original code was already correct.**  The provided code snippet was functionally correct and would calculate the sum of a list of numbers.  There was nothing inherently wrong with it.\n",
            "\n",
            "*   **The \"fix\" is primarily adding a docstring.**  I've added a docstring (the triple-quoted string) to explain what the function does, what arguments it takes, and what it returns. This is good practice for code readability and maintainability.  It helps others (and your future self) understand the purpose of the function quickly.\n",
            "\n",
            "*   **No functional changes were needed.** The core logic of the original code (initializing `total` to 0, iterating through the list, and adding each element to `total`) is the correct and efficient way to sum a list.\n",
            "\n",
            "--- ENGINEERED PROMPT (GPT-4o) ---\n",
            "### Revised Code\n",
            "```python\n",
            "def sum_list(l):\n",
            "    \"\"\"\n",
            "    Calculate the sum of a list of numbers.\n",
            "\n",
            "    Args:\n",
            "        l (list): A list of numerical values.\n",
            "\n",
            "    Returns:\n",
            "        int or float: The sum of the numbers in the list.\n",
            "    \"\"\"\n",
            "    return sum(l)\n",
            "```\n",
            "\n",
            "### Critique\n",
            "The original code snippet iteratively sums the elements of the list using a for loop, which is less efficient compared to utilizing Python's built-in `sum()` function. This built-in method is optimized for performance and readability, allowing for cleaner code. Additionally, the original code lacks a docstring, which is essential for understanding the function's purpose and usage. A significant logical bug in the original code is the absence of input validation; if the input list contains non-numeric types, it will raise a `TypeError` during summation. The refactored code addresses efficiency and clarity but does not include input validation, which could be an area for further improvement.\n"
          ]
        }
      ],
      "source": [
        "code_snippet = \"def sum_list(l): total = 0; for i in l: total += i; return total\"\n",
        "\n",
        "\n",
        "# ---  PROMPT TEST ---\n",
        "code_prompt = f\"Fix this code and tell me what is wrong: {code_snippet}\"\n",
        "print(\"--- PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": code_prompt}))\n",
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT ---\n",
        "roles_code_prompt = generate_roles_prompt(\n",
        "    role=\"A Senior Python Developer specializing in clean, efficient code.\",\n",
        "    objective=\"Refactor the provided code snippet for efficiency (using built-in methods), add a docstring, and identify a single major bug in the logic (if any).\",\n",
        "    limitations=\"Do not change the function name. Output the revised code first, then provide a single paragraph critique.\",\n",
        "    style=\"Formal tone. Output must be in two sections: 'Revised Code' and 'Critique'.\",\n",
        "    query=f\"Here is the code: {code_snippet}\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (GPT-4o) ---\")\n",
        "print(create_roles_chain(openai_model).invoke({\"input\": roles_code_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnnsy1qzlO9c"
      },
      "source": [
        "Prompt Set 3: Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8KBfWTFlfJK",
        "outputId": "9fa093fe-98d8-42b5-facf-2a0a419b4d10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\n",
            "Alright, let's dissect this week's AAPL charade. The stock exhibited a rather uninspired sideways drift, oscillating within a narrow range between $175 and $178. Volume remained anemic, suggesting a lack of conviction from either the bulls or the bears. The 50-day moving average continues to act as a weak support, but the price action is flirting dangerously close to breaching it. The Relative Strength Index (RSI) hovers around 50, indicating a neutral momentum. Furthermore, the stochastic oscillator is showing a potential bearish crossover, which, combined with the lack of upward momentum, suggests a likely short-term correction. I predict a modest decline, potentially testing the $173 level, before any significant rebound. This is, of course, assuming the market doesn't conjure up some unforeseen catalyst to disrupt this predictable inertia.\n"
          ]
        }
      ],
      "source": [
        "roles_finance_prompt = generate_roles_prompt(\n",
        "    role=\"A highly skeptical Senior Financial Analyst.\",\n",
        "    objective=\"Analyze the past week's fictional performance of AAPL stock based on technical analysis, and make a plausible, short-term prediction.\",\n",
        "    limitations=\"Do not use real-time search. Analysis must be a single paragraph.\",\n",
        "    style=\"Highly technical, formal, and use the term 'stochastic' at least once.\",\n",
        "    query=\"Perform the analysis.\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": roles_finance_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1gW5OfFlrO-"
      },
      "source": [
        "Prompt Set 4: Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI9pb1ljl583",
        "outputId": "97ca50fa-f08d-4a35-a14c-6704ca93f826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- NORMAL PROMPT (GPT-4o) ---\n",
            "Acme Corp announced a Q3 revenue increase of $45 million, up from $30 million last year, on October 15th. The CEO indicated a 20% increase in hiring starting January 1st, driven by a successful digital marketing campaign launched in July. Following the announcement, the stock rose by $5 per share within 48 hours.\n",
            "\n",
            "--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\n",
            "On October 15th, Acme Corp reported Q3 revenue of $45 million, exceeding the previous year's $30 million. Hiring is projected to increase by 20% starting January 1st.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "long_text = \"\"\"\n",
        "The company, Acme Corp, announced its Q3 earnings on October 15th, reporting a staggering\n",
        "revenue increase of $45 million, surpassing last year's figure of $30 million.\n",
        "The CEO stated that hiring will increase by 20% in the new year, starting January 1st.\n",
        "This growth is primarily attributed to their successful digital marketing campaign launched in July.\n",
        "The stock reacted positively, climbing $5 per share within 48 hours of the announcement.\n",
        "\"\"\"\n",
        "\n",
        "# ---  PROMPT TEST ---\n",
        "bad_summarize_prompt = f\"Summarize this paragraph: {long_text}\"\n",
        "print(\"--- NORMAL PROMPT (GPT-4o) ---\")\n",
        "print(create_roles_chain(openai_model).invoke({\"input\": bad_summarize_prompt}))\n",
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT ---\n",
        "roles_summarize_prompt = generate_roles_prompt(\n",
        "    role=\"A News Editor reviewing a financial wire report.\",\n",
        "    objective=\"Summarize the text, focusing only on reported dates and financial figures.\",\n",
        "    limitations=\"Must be exactly 2 sentences long. Include no opinion or analysis.\",\n",
        "    style=\"Objective and journalistic.\",\n",
        "    query=f\"Summarize this text: {long_text}\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": roles_summarize_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byGOIbtlmDhO"
      },
      "source": [
        "Prompt Set 5: Topic Classification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k93Mi6ZkmObz",
        "outputId": "a1ed6a80-6a38-4114-84d0-95d4c31b2a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- BAD PROMPT (Gemini 2.5 Flash) ---\n",
            "This text is about **a positive change in the performance of a computer's graphics card (GPU)**. Specifically:\n",
            "\n",
            "*   **Driver updates:** Software updates for the GPU.\n",
            "*   **Fixed latency issues:** Resolved delays or lag in processing data.\n",
            "*   **Improved frame rates substantially:** Made the game or application run smoother and faster, leading to a better visual experience.\n",
            "\n",
            "In short, the text describes how updating the GPU's drivers has improved its performance.\n",
            "\n",
            "--- ENGINEERED PROMPT (GPT-4o) ---\n",
            "Hardware\n"
          ]
        }
      ],
      "source": [
        "\n",
        "classification_text = \"The latest driver updates fixed the latency issues on the GPU, improving frame rates substantially.\"\n",
        "\n",
        "# --- NORMAL/BAD PROMPT TEST ---\n",
        "bad_classify_prompt = f\"What is this text about: {classification_text}\"\n",
        "print(\"--- BAD PROMPT (Gemini 2.5 Flash) ---\")\n",
        "print(create_roles_chain(gemini_model).invoke({\"input\": bad_classify_prompt}))\n",
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT (Using Examples for format) ---\n",
        "roles_classify_prompt = generate_roles_prompt(\n",
        "    role=\"A Data Scientist performing classification.\",\n",
        "    objective=\"Classify the following text into one of these categories: [Hardware, Software, Finance].\",\n",
        "    limitations=\"Output must be a single word and case-sensitive (e.g., 'Hardware').\",\n",
        "    style=\"Raw text output. Use the following example:\",\n",
        "    query=f\"\"\"\n",
        "    Example Input: \"The market closed high.\"\n",
        "    Example Output: Finance\n",
        "\n",
        "    Classify this input: \"{classification_text}\"\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (GPT-4o) ---\")\n",
        "print(create_roles_chain(openai_model).invoke({\"input\": roles_classify_prompt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8mq7xNqml8G"
      },
      "source": [
        "Prompt Set 6: Sentiment Analysis\n",
        "\n",
        "Determine if the customer sentiment is positive, neutral, or negative.\n",
        "\n",
        "Output must be a JSON object with keys sentiment and confidence (1-100)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UViLmIghfH8H",
        "outputId": "bfd79562-4dbd-4750-e0ab-676361c6bf5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ENGINEERED PROMPT (GPT-4o) ---\n",
            "{\n",
            "  \"sentiment\": \"Negative\",\n",
            "  \"confidence\": 85\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "sentiment_review = \"The product works exactly as advertised, but the delivery took far too long, making the overall experience frustrating.\"\n",
        "\n",
        "\n",
        "# --- R.O.L.E.S. ENGINEERED PROMPT ---\n",
        "roles_sentiment_prompt = generate_roles_prompt(\n",
        "    role=\"A Customer Service Manager focused on customer feedback.\",\n",
        "    objective=\"Analyze the provided review text to determine overall customer sentiment (Positive, Negative, or Neutral).\",\n",
        "    limitations=\"Only use the exact labels: 'Positive', 'Negative', or 'Neutral'.\",\n",
        "    style=\"Strictly output a JSON object.\",\n",
        "    query=f\"\"\"\n",
        "    Analyze this review: \"{sentiment_review}\"\n",
        "\n",
        "    Required JSON format:\n",
        "    {{\n",
        "      \"sentiment\": \"<LABEL>\",\n",
        "      \"confidence\": \"<1-100 score>\"\n",
        "    }}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENGINEERED PROMPT (GPT-4o) ---\")\n",
        "print(create_roles_chain(openai_model).invoke({\"input\": roles_sentiment_prompt}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We8oL-zMlMMY"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}